{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "DzfJfOi8WRJp",
   "metadata": {
    "id": "DzfJfOi8WRJp"
   },
   "source": [
    "# üìù Pattern Recognition & Biometrics. TP Apprentissage profond ou *Deep Learning*\n",
    "\n",
    "Par Omar Galarraga et Sonia Garcia"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "EWDOsRaVWRJx",
   "metadata": {
    "id": "EWDOsRaVWRJx"
   },
   "source": [
    "Dans ce travail pratique, nous verrons l'int√©r√™t des mod√®les d'apprentissage profond (*Deep Learning*) en les comparant √† des mod√®les de *Machine Learning* classiques (non profonds).  \n",
    "\n",
    "Nous voudrons identifier les animaux et les objets √† partir des images de la base de donn√©es CIFAR 10 (https://www.cs.toronto.edu/%7Ekriz/cifar.html). \n",
    "\n",
    "Commen√ßons par charger les donn√©es...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tsw6rkteKky7",
   "metadata": {
    "id": "tsw6rkteKky7"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow import keras\n",
    "from keras import datasets\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, MaxPooling2D, Conv2D, Flatten\n",
    "\n",
    "(data_train, target_train), (data_test, target_test) = tf.keras.datasets.cifar10.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "shFvcYh2K0fn",
   "metadata": {
    "id": "shFvcYh2K0fn"
   },
   "source": [
    "A mani√®re d'exemple, nous afficherons 3 images de chaque classe :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0Gzklw6ILcRd",
   "metadata": {
    "id": "0Gzklw6ILcRd"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "classes = ['avion', 'voiture', 'oiseau', 'chat', 'cerf', \n",
    "            'chien', 'crapaud', 'cheval', 'bateau', 'camion']\n",
    "\n",
    "# Normalisation des valeurs des pixels entre 0 and 1\n",
    "data_train, data_test = data_train / 255.0, data_test / 255.0\n",
    "\n",
    "n_images = 3\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "for i in range(len(classes)):\n",
    "  idx = np.where(target_train == i)\n",
    "  #print(idx)\n",
    "  for j in range(n_images): \n",
    "    plt.subplot(len(classes),n_images,n_images*i+j+1)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.grid(False)\n",
    "    plt.imshow(data_train[idx[0][j]])\n",
    "    if j == 0:\n",
    "        plt.ylabel(classes[target_train[idx[0][j]][0]])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ot2tF6FsWRJ0",
   "metadata": {
    "id": "ot2tF6FsWRJ0"
   },
   "source": [
    "## Partie I. *Feedforward Neural Network* √† une seule couche cach√©e\n",
    "\n",
    "Nous construisons un r√©seau de neurones avec $N_{H_l} = 1$ couche cach√©e et $m=32$ neurones dans la couche cach√©e. \n",
    "\n",
    "* I.1) Si on consid√®re les images \"brutes\", quelle est la dimension de la \"couche\" d'entr√©e ?\n",
    "\n",
    "***R√©ponse:*** La dimension de couche d'entr√©e 3072\n",
    "* I.2) Quelle est la performance du mod√®le sur les ensembles d'apprentissage et de test ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3wRDkO5zWRJ3",
   "metadata": {
    "id": "3wRDkO5zWRJ3"
   },
   "outputs": [],
   "source": [
    "m=32\n",
    "refmodel = tf.keras.Sequential()\n",
    "refmodel.add(Flatten(input_shape=data_train[0].shape))\n",
    "refmodel.add(Dense(m, activation='sigmoid'))\n",
    "refmodel.add(Dense(len(classes)))\n",
    "\n",
    "refmodel.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])\n",
    "history1 = refmodel.fit(data_train, target_train, epochs=20, \n",
    "                        validation_data=(data_test, target_test), batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6085cf5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(history1.history['accuracy'], label='Training Accuracy')\n",
    "plt.plot(history1.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.title('Model Accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "print(\"Final Training Accuracy: \", history1.history['accuracy'][-1])\n",
    "print(\"Final Validation Accuracy: \", history1.history['val_accuracy'][-1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ma0pJiRGWRJ_",
   "metadata": {
    "id": "ma0pJiRGWRJ_"
   },
   "source": [
    "Optimisez le nombre de neurones dans la couche cach√©e dans l'intervalle $m = \\left[2,128\\right]$\n",
    "* I.3) Quelle est la valeur optimale de $ m $ ?\n",
    "* I.4) Quelles sont les nouvelles performances en apprentissage et en test ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "I5wcNSeVWRKA",
   "metadata": {
    "id": "I5wcNSeVWRKA"
   },
   "outputs": [],
   "source": [
    "#Ecrivez votre code ici\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Supposons que data_train, target_train, data_test, et target_test sont d√©j√† d√©finis\n",
    "m_values = list(range(2,129))\n",
    "train_accuracies = []\n",
    "test_accuracies = []\n",
    "\n",
    "for m in m_values:\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Flatten(input_shape=data_train[0].shape),\n",
    "        tf.keras.layers.Dense(m, activation='sigmoid'),\n",
    "        tf.keras.layers.Dense(len(classes))\n",
    "    ])\n",
    "    \n",
    "    model.compile(optimizer='adam',\n",
    "                  loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    history = model.fit(data_train, target_train, epochs=20,\n",
    "                        validation_data=(data_test, target_test), batch_size=64, verbose=0)\n",
    "    \n",
    "    train_accuracy = history.history['accuracy'][-1]\n",
    "    test_accuracy = history.history['val_accuracy'][-1]\n",
    "    train_accuracies.append(train_accuracy)\n",
    "    test_accuracies.append(test_accuracy)\n",
    "\n",
    "# Trouver la meilleure performance\n",
    "max_index = np.argmax(test_accuracies)\n",
    "optimal_m = m_values[max_index]\n",
    "\n",
    "# Affichage des r√©sultats\n",
    "print(f\"Optimal number of neurons m: {optimal_m}\")\n",
    "print(f\"Training Accuracy at m={optimal_m}: {train_accuracies[max_index]:.4f}\")\n",
    "print(f\"Test Accuracy at m={optimal_m}: {test_accuracies[max_index]:.4f}\")\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(m_values, train_accuracies, label='Training Accuracy')\n",
    "plt.plot(m_values, test_accuracies, label='Test Accuracy')\n",
    "plt.xlabel('Number of Neurons in Hidden Layer (m)')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Model Performance vs. Number of Neurons')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tHuhNmvRomRj",
   "metadata": {
    "id": "tHuhNmvRomRj"
   },
   "source": [
    "Montrez la matrice de confusion du mod√®le optimal sur l'ensemble de test\n",
    "\n",
    "* I.4) Quelle est la pr√©diction \"pr√©f√©r√©e\" du mod√®le ?\n",
    "* I.5) Quelle est la classe la mieux pr√©dite ? Justifiez bri√®vement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "VHDli7m-ott4",
   "metadata": {
    "id": "VHDli7m-ott4"
   },
   "outputs": [],
   "source": [
    "#Ecrivez votre code ici\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assurez-vous que model est le mod√®le optimal obtenu pr√©c√©demment\n",
    "predictions = model.predict(data_test)\n",
    "predicted_classes = np.argmax(predictions, axis=1)\n",
    "\n",
    "# Calcul de la matrice de confusion\n",
    "cm = confusion_matrix(target_test, predicted_classes)\n",
    "print(\"Matrice de confusion :\\n\", cm)\n",
    "\n",
    "# Visualisation de la matrice de confusion\n",
    "plt.figure(figsize=(10, 7))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "plt.xlabel('Predicted Labels')\n",
    "plt.ylabel('True Labels')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()\n",
    "\n",
    "# Analyse pour trouver la pr√©diction pr√©f√©r√©e et la classe la mieux pr√©dite\n",
    "most_predicted_class = np.argmax(np.sum(cm, axis=0))\n",
    "best_predicted_class = np.argmax(cm.diagonal() / np.sum(cm, axis=1))\n",
    "\n",
    "print(f\"Pr√©diction pr√©f√©r√©e du mod√®le (classe la plus fr√©quemment pr√©dite): {most_predicted_class}\")\n",
    "print(f\"Classe la mieux pr√©dite (plus haute pr√©cision): {best_predicted_class}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "J1IbednbWRKI",
   "metadata": {
    "id": "J1IbednbWRKI"
   },
   "source": [
    "## Partie II. *Feedforward Neural Networks* √† plusieurs couches\n",
    "\n",
    "Construisez et entra√Ænez un r√©seau de neurones type Perceptron Multicouche (PMC) √† $N_{H_l} = 3$ couches cach√©es et avec $m_1=32, m_2=16, m_3 = 16$ neurones respectivement pour les couches cach√©es 1, 2 et 3.\n",
    "\n",
    "* II.1) Combien de param√®tres sont-ils optimis√©s au total ? \n",
    "* II.2) Quelle est la performance en apprentissage et validation ?\n",
    "* II.3) Montrez la matrice de confusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "SI7tyTgwWRKU",
   "metadata": {
    "id": "SI7tyTgwWRKU"
   },
   "outputs": [],
   "source": [
    "#Ecrivez votre code ici\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "HxRPx84AWRKZ",
   "metadata": {
    "id": "HxRPx84AWRKZ"
   },
   "source": [
    "## Partie III. R√©seaux de neurones convolutionnels (*CNN*)\n",
    "\n",
    "\n",
    "Cr√©ez un CNN √† l'image de *LeNet-5* (LeCun et al., 1998), qui consiste en une double alternance entre une couche convolutionnelle ($m=32$, kernel de $5x5$) et une couche de *Maximum Pooling* (kernel de $2x2$), suivie d'une couche dense ($m=64$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6DUG0ViJWRKa",
   "metadata": {
    "id": "6DUG0ViJWRKa"
   },
   "outputs": [],
   "source": [
    "#Ecrivez votre code ici\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "CIQyWFE6s6RY",
   "metadata": {
    "id": "CIQyWFE6s6RY"
   },
   "source": [
    "\n",
    "* III.1) Combien de param√®tres arrivent-ils en entr√©e de la couche dense ?\n",
    "* III.2) Combien de param√®tres sont-ils entra√Æn√©s au total ?\n",
    "* III.3) Quelle est la performance d'apprentissage et de validation ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ymo2AiogWRKa",
   "metadata": {
    "id": "ymo2AiogWRKa"
   },
   "outputs": [],
   "source": [
    "#Ecrivez votre code ici\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "oJ76nnyCoaSh",
   "metadata": {
    "id": "oJ76nnyCoaSh"
   },
   "source": [
    "## Partie IV. Transfer learning\n",
    "\n",
    "Avec l'immense progression r√©cente du Deep Learning, il est souvent plus int√©ressant de cr√©er un mod√®le √† partir d'un autre mod√®le d√©j√† entra√Æn√© et l'ajuster (*fine-tune*) selon la t√¢che sp√©cifique que l'on veut r√©aliser. Actuellement un large catalogue de mod√®les tr√®s complexes entra√Æn√©s sur des bases de donn√©es de tr√®s grande taille est √† disposition du public g√©n√©ral.   \n",
    "\n",
    "Commen√ßons par cr√©er un r√©seau type *ResNet-18* (He et al., 2015) pr√©-entra√Æn√© sur la base de donn√©es *ImageNet*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "CnFl4Qdmqnfu",
   "metadata": {
    "id": "CnFl4Qdmqnfu"
   },
   "outputs": [],
   "source": [
    "#import torch \n",
    "from tensorflow.keras.applications import ResNet50\n",
    "\n",
    "#resnet = torch.hub.load('pytorch/vision:v0.10.0', 'resnet18', pretrained=True) \n",
    "\n",
    "ResNet50_model = ResNet50(\n",
    "    include_top=False, \n",
    "    weights='imagenet', \n",
    "    input_shape= (32, 32, 3),\n",
    "    pooling='avg')\n",
    "\n",
    "tl_cnn = Sequential()\n",
    "tl_cnn.add(ResNet50_model)\n",
    "tl_cnn.add(Flatten())\n",
    "tl_cnn.add(Dense(10, activation='softmax'))\n",
    "tl_cnn.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Zra8wbQubyQV",
   "metadata": {
    "id": "Zra8wbQubyQV"
   },
   "source": [
    "Maintenant on ajuste le mod√®le pr√©-entra√Æn√© √† notre probl√®me sp√©cifique (*fine-tuning*) \n",
    "(Il est pr√©f√©rable d'utiliser un GPU. Par exemple, Google Colab, Codesphere, Gradient by Paperspace, Kaggle, proposent des GPU gratuitement)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "iE3I1LDObX_Z",
   "metadata": {
    "id": "iE3I1LDObX_Z"
   },
   "outputs": [],
   "source": [
    "tl_cnn.compile(optimizer='adam', loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),metrics=['accuracy'])\n",
    "history4 = tl_cnn.fit(data_train, target_train, epochs=20, validation_data=(data_test, target_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "C2zn3gc1jZnd",
   "metadata": {
    "id": "C2zn3gc1jZnd"
   },
   "source": [
    "Remplissez le tableau ci-dessous selon les diff√©rentes m√©thodes de classification test√©es.\n",
    "\n",
    "| Classifieur        | Perf Apprentissage | Perf Test | \n",
    "|--------------------|--------------------|-----------|\n",
    "| *FNN* simple       |  |  |  \n",
    "| *FNN* profond      |  |  | \n",
    "| *CNN*              |  |  | \n",
    "| *ResNet50*         |  |  | \n",
    "\n",
    "* IV.1) Quel est le meilleur classifieur ? Justifiez\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5-qeMvzydH82",
   "metadata": {
    "id": "5-qeMvzydH82"
   },
   "source": [
    "IV.2) Pendant quelques ann√©es les mod√®les ResNet ont √©t√© le *silver standard* dans la classification d'images. Plus r√©cemment, une autre famille de mod√®les appel√©e *ConvNet* (Liu et al., 2022) a montr√© des meilleures performances. Refaite l'exp√©rience pr√©c√©dente en partant d'un mod√®le *ConvNet* pr√©-entra√Æn√©. \n",
    "(Il est pr√©f√©rable d'utiliser un GPU. Par exemple, Google Colab, Codesphere, Gradient by Paperspace, Kaggle, proposent des GPU gratuitement)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lMI-FQzbf7oN",
   "metadata": {
    "id": "lMI-FQzbf7oN"
   },
   "outputs": [],
   "source": [
    "#Ecrivez votre code ici"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7aef65e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "jupytext": {
   "main_language": "python"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "nbreset": "https://raw.githubusercontent.com/INRIA/scikit-learn-mooc/main/notebooks/01_tabular_data_exploration_ex_01.ipynb"
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
